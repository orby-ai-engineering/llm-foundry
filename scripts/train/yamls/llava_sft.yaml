
max_seq_len: 16384
global_seed: 17

# Run Name
run_name: # If left blank, will be read from env var $COMPOSER_RUN_NAME

# Model
model:
  name: llava_causal_lm
  pretrained_model_name_or_path: llava-hf/llava-v1.6-mistral-7b-hf
  use_flash_attention_2: true
  freeze_language_model: false
  pretrained: true

# Tokenizer
tokenizer:
  name: llava-hf/llava-v1.6-mistral-7b-hf
  kwargs:
    padding_side: left

# Dataloaders
train_loader:
  name: finetuning
  dataset:
    ############
     remote: s3://orby-osu-va/mds_datasets/test_100k_action_predict/eval
    local: ./train-data
    download_timeout: 300 
    ############
    shuffle: true
    max_seq_len: ${max_seq_len}
    allow_pad_trimming: false
    decoder_only_format: true
  drop_last: true
  num_workers: 8
  pin_memory: false
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

eval_loader:
  name: finetuning
  dataset:
    ############
    remote: s3://orby-osu-va/mds_datasets/test_100k_action_predict/eval
    local: ./eval-data
    ############
    shuffle: false
    max_seq_len: ${max_seq_len}
    allow_pad_trimming: false
    decoder_only_format: true
  drop_last: true
  num_workers: 8
  pin_memory: false
  prefetch_factor: 2
  persistent_workers: true
  timeout: 0

# Optimization
scheduler:
  name: linear_decay_with_warmup # linear no warmup is HF default which dolly used
  t_warmup: 10ba # add some warmup though, seems to help with MPT
  alpha_f: 0

optimizer:
  # Based on Dolly
  name: decoupled_adamw
  lr: 1.0e-5
  betas:
    - 0.9
    - 0.999
  eps: 1.0e-8
  weight_decay: 0

algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0

max_duration: 10ep # 2-3 epochs seems like the sweet spot
eval_interval: 1ep
# eval_subset_num_batches: -1
eval_first: true 
global_train_batch_size: 96

# System
seed: ${global_seed}
device_eval_batch_size: 1 # TODO(sanjari): allow non one evaluation batch sizes
device_train_microbatch_size: 3
precision: amp_bf16

# # In-context learning evaluation
icl_tasks: eval/yamls/orby_tasks.yaml # or use tasks_light.yaml
icl_subset_num_batches: 16 # -1, or omit this key entirely, to evaluate on all batches. Using 100 samples from each task for evaluation.
eval_gauntlet: 'eval/yamls/orby_eval_gauntlet.yaml'
icl_seq_len: 4096

# FSDP
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: PURE 
  # mixed_precision: DEFAULT 
  activation_checkpointing: true
  activation_checkpointing_reentrant: false 
  activation_cpu_offload: false
  limit_all_gathers: true
  verbose: false 

# Logging
progress_bar: true
log_to_console: true
console_log_interval: 5ba

callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}

loggers:
  # This requires uploading Databricks creds to MosaicML platform as a secret
  wandb:
    project: action_pred
    entity: orby-osu

# Checkpoint to local filesystem or remote object store
save_interval: 5ep
save_num_checkpoints_to_keep: 1
# This requires uploading GCP creds to MosaicML platform as a secret
save_folder: s3://orby-osu/full-finetuning/{run_name}/checkpoints
